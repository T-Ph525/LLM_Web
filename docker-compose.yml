version: '3'
services:
  nginx:
    build: .
    ports:
      - "80:80"
      - "443:443"
    networks:
      - gateway_network
  text-generation-inference:
    image: ghcr.io/huggingface/text-generation-inference:latest
    command: ["--max-total-tokens", "4096", "--max-input-length", "4000", "--max-batch-prefill-tokens", "4096", "--quantize", "gptq", "--model-id", "TheBloke/WizardCoder-Python-34B-V1.0-GPTQ"]
    volumes:
      - $PWD/models:/data
    container_name: text-generation-inference
    deploy:
      resources:
        reservations:
          devices:
          - driver: "nvidia"
            device_ids: ["0"]
            capabilities: [gpu]
          memory: 1g
    networks:
      - gateway_network
networks:
  gateway_network:
    driver: bridge
